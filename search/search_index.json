{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"Supervised and Unsupervised Machine Learning \ud83d\udd17 This is the Supervised and Unsupervised Learning section of the Algorithms in Machine Learning class at ISAE-Supaero. The Neural Networks and Deep Learning topics have a class of their own and won't be covered here. Home Github repository Syllabus \ud83d\udd17 This course offers a discovery of the landscape of Machine Learning through some key algorithms. It follows the Statistical Foundations of Machine Learning part and will be followed by the Neural Networks and Deep Learning part . Over 25 hours, we will cover the landscape of Supervised and Unsupervised Machine Learning by taking four different points of view and formalisms: the geometrical perspective, the probabilistic (Bayesian) perspective, the connectionnist perspective (which we will postpone to the next class on Deep Learning), and the ensemble perspective. The class is organized in 9 sessions: Introduction. Introduction to the landscape and workflow of Machine Learning, a few words on specificities of Unsupervised Learning, the importance of data preprocessing (1 session). A geometrical approach to Machine Learning. Support Vector Machines, the bias/variance tradeoff and a bit of kernel theory. A Bayesian perspective on Machine Learning. Naive Bayes Classification and Gaussian Processes, followed by an in-depth class on Surrogate Modeling and Bayesian Optimization. Committee learning methods. Decision Trees and Boosting, followed by an in-depth class on Gradient Boosting. Bagging and Random Forests, followed by an in-depth class on anomaly detection. A \"perspectives\" final session. Explainability in Machine Learning. The pedagogy taken mixes voluntarily hands-on practice in Python with theoretical and mathematical understanding of the methods. At the end of the course you will be able to make an informed choice between the main families of ML algorithms, depending on the problem at hand. You will have an understanding of the algorithmic and mathematical properties of each family of methods and you will have a basic practical knowledge of the Scikit-Learn library. Bibliography \ud83d\udd17 The Elements of Statistical Learning. T. Hastie, R. Tibshirani, J. Friedman. Springer Series in Statistics. https://web.stanford.edu/~hastie/ElemStatLearn/","title":"Supervised and Unsupervised Machine Learning"},{"location":"index.html#supervised-and-unsupervised-machine-learning","text":"This is the Supervised and Unsupervised Learning section of the Algorithms in Machine Learning class at ISAE-Supaero. The Neural Networks and Deep Learning topics have a class of their own and won't be covered here. Home Github repository","title":"Supervised and Unsupervised Machine Learning"},{"location":"index.html#syllabus","text":"This course offers a discovery of the landscape of Machine Learning through some key algorithms. It follows the Statistical Foundations of Machine Learning part and will be followed by the Neural Networks and Deep Learning part . Over 25 hours, we will cover the landscape of Supervised and Unsupervised Machine Learning by taking four different points of view and formalisms: the geometrical perspective, the probabilistic (Bayesian) perspective, the connectionnist perspective (which we will postpone to the next class on Deep Learning), and the ensemble perspective. The class is organized in 9 sessions: Introduction. Introduction to the landscape and workflow of Machine Learning, a few words on specificities of Unsupervised Learning, the importance of data preprocessing (1 session). A geometrical approach to Machine Learning. Support Vector Machines, the bias/variance tradeoff and a bit of kernel theory. A Bayesian perspective on Machine Learning. Naive Bayes Classification and Gaussian Processes, followed by an in-depth class on Surrogate Modeling and Bayesian Optimization. Committee learning methods. Decision Trees and Boosting, followed by an in-depth class on Gradient Boosting. Bagging and Random Forests, followed by an in-depth class on anomaly detection. A \"perspectives\" final session. Explainability in Machine Learning. The pedagogy taken mixes voluntarily hands-on practice in Python with theoretical and mathematical understanding of the methods. At the end of the course you will be able to make an informed choice between the main families of ML algorithms, depending on the problem at hand. You will have an understanding of the algorithmic and mathematical properties of each family of methods and you will have a basic practical knowledge of the Scikit-Learn library.","title":"Syllabus"},{"location":"index.html#bibliography","text":"The Elements of Statistical Learning. T. Hastie, R. Tibshirani, J. Friedman. Springer Series in Statistics. https://web.stanford.edu/~hastie/ElemStatLearn/","title":"Bibliography"},{"location":"anomaly.html","text":"Anomaly detection \ud83d\udd17","title":"Anomaly Detection"},{"location":"anomaly.html#anomaly-detection","text":"","title":"Anomaly detection"},{"location":"bagging.html","text":"Bagging \ud83d\udd17","title":"Bagging"},{"location":"bagging.html#bagging","text":"","title":"Bagging"},{"location":"boost.html","text":"Boosting \ud83d\udd17","title":"Boosting"},{"location":"boost.html#boosting","text":"","title":"Boosting"},{"location":"gp.html","text":"Gaussian Processes \ud83d\udd17","title":"Gaussian Processes"},{"location":"gp.html#gaussian-processes","text":"","title":"Gaussian Processes"},{"location":"intro.html","text":"Introduction to Machine Learning \ud83d\udd17 Home Github repository This class provides an introduction to the landscape, zoo, and workflow of Machine Learning. Slides","title":"Introduction to ML"},{"location":"intro.html#introduction-to-machine-learning","text":"Home Github repository This class provides an introduction to the landscape, zoo, and workflow of Machine Learning. Slides","title":"Introduction to Machine Learning"},{"location":"mlc.html","text":"An application of SVMs to multi-label classification \ud83d\udd17 Home Github repository This short application session proposes to use SVMs as a building block for Multi-Label Classification. It serves both as a practice session for the previous class and as an introduction to Multi-Label Classification. Notebook source Notebook on Colab References \ud83d\udd17 J. Read, P. Reutemann, B. Pfahringer, and Geoff Holmes. MEKA: A multi-label/multi-target extension to Weka . Journal of Machine Learning Research, 17(21):1\u20135, 2016. J. Read, B. Pfahringer, G. Holmes, and E. Frank. Classifier chains for multi-label classification . Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 254\u2013269, 2009. G. Tsoumakas and I. Katakis. Multi-label classification: An overview . International Journal on Data Warehousing and Mining, 3(3):1\u201313, 2007. G. Tsoumakas, I. Katakis, and I. Vlahavas. Mining multi-label data . Data mining and knowledge discovery handbook, pages 667\u2013685. Springer, 2010. G. Tsoumakas, I. Katakis, and I. Vlahavas. Random k-labelsets for multi-label classification . IEEE Transactions on Knowledge and Data Engineering, 23(7):1079-1089, 2011.","title":"An application of SVMs to multi-label classification"},{"location":"mlc.html#an-application-of-svms-to-multi-label-classification","text":"Home Github repository This short application session proposes to use SVMs as a building block for Multi-Label Classification. It serves both as a practice session for the previous class and as an introduction to Multi-Label Classification. Notebook source Notebook on Colab","title":"An application of SVMs to multi-label classification"},{"location":"mlc.html#references","text":"J. Read, P. Reutemann, B. Pfahringer, and Geoff Holmes. MEKA: A multi-label/multi-target extension to Weka . Journal of Machine Learning Research, 17(21):1\u20135, 2016. J. Read, B. Pfahringer, G. Holmes, and E. Frank. Classifier chains for multi-label classification . Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 254\u2013269, 2009. G. Tsoumakas and I. Katakis. Multi-label classification: An overview . International Journal on Data Warehousing and Mining, 3(3):1\u201313, 2007. G. Tsoumakas, I. Katakis, and I. Vlahavas. Mining multi-label data . Data mining and knowledge discovery handbook, pages 667\u2013685. Springer, 2010. G. Tsoumakas, I. Katakis, and I. Vlahavas. Random k-labelsets for multi-label classification . IEEE Transactions on Knowledge and Data Engineering, 23(7):1079-1089, 2011.","title":"References"},{"location":"nbc.html","text":"Naive Bayes Classification \ud83d\udd17","title":"Naive Bayes Classification"},{"location":"nbc.html#naive-bayes-classification","text":"","title":"Naive Bayes Classification"},{"location":"nn.html","text":"Neural networks \ud83d\udd17 This part of the class grew and now has its own full course. It is the upcoming Neural Networks and Deep Learning class.","title":"Neural networks"},{"location":"nn.html#neural-networks","text":"This part of the class grew and now has its own full course. It is the upcoming Neural Networks and Deep Learning class.","title":"Neural networks"},{"location":"preproc.html","text":"The importance of data pre-processing \ud83d\udd17 Home Github repository Through an example of text data preparation, this class illustrates why it is crucial to think about data preprocessing before starting thinking about algorithms in ML. Notebook ( colab )","title":"The importance of data preprocessing"},{"location":"preproc.html#the-importance-of-data-pre-processing","text":"Home Github repository Through an example of text data preparation, this class illustrates why it is crucial to think about data preprocessing before starting thinking about algorithms in ML. Notebook ( colab )","title":"The importance of data pre-processing"},{"location":"rf.html","text":"Random Forests \ud83d\udd17","title":"Random Forests"},{"location":"rf.html#random-forests","text":"","title":"Random Forests"},{"location":"surrog.html","text":"Surrogate Modeling and Bayesian Optimization \ud83d\udd17","title":"Surrogate Modeling and Bayesian Optimization"},{"location":"surrog.html#surrogate-modeling-and-bayesian-optimization","text":"","title":"Surrogate Modeling and Bayesian Optimization"},{"location":"svm.html","text":"Support Vector Machines, the bias-variance tradeoff and a bit of kernel theory \ud83d\udd17 Home Github repository This class takes a geometrical approach to Machine Learning through the prism of Support Vector Machines. It covers linear classifiers for data separation first. On the way, it introduces the bias/variance tradeoff. Then it presents a bit of kernel theory and applies it in linear classifiers to reach non-linear SVMs. It then provides perspectives on multi-class classification, support vector regression and density estimation with one-class SVM. Two full practical examples are provided at the end. Notebook ( colab ) Pre-class refresher activities and solution Summary card Lecture notes References \ud83d\udd17 On the general theory of SVMs for classification: A tutorial on Support Vector Machines for Pattern Recognition. C. J. C. Burges, Data Mining and Knowledge Discovery , 2 , 131-167, (1998). On support vector regression (and its extension to $\\nu$-SVR): A tutorial on Support Vector Regression. A. J. Smola and B. Sch\u00f6lkopf, Journal of Statistics and Computing , 14 (3), 199-222, (2004). New support vector algorithms. B. Sch\u00f6lkopf, A. J. Smola, R. C. Williamson, and P. L. Bartlett. Neural computation , 12 (5), 1207-1245, (2000). On One-Class SVMs: Support vector method for novelty detection. B. Sch\u00f6lkopf, R. C. Williamson, A. J. Smola, J. Shawe-Taylor, and John C. Platt. Neural Information Processing Systems , 12 , 582-588, (1999). On multi-class SVMs: On the algorithmic implementation of multiclass kernel-based vector machines. K. Crammer and Y. Singer. Journal of machine learning research , 2 , 265-292, (2001).","title":"SVMs, the bias-variance tradeoff and a bit of kernel theory"},{"location":"svm.html#support-vector-machines-the-bias-variance-tradeoff-and-a-bit-of-kernel-theory","text":"Home Github repository This class takes a geometrical approach to Machine Learning through the prism of Support Vector Machines. It covers linear classifiers for data separation first. On the way, it introduces the bias/variance tradeoff. Then it presents a bit of kernel theory and applies it in linear classifiers to reach non-linear SVMs. It then provides perspectives on multi-class classification, support vector regression and density estimation with one-class SVM. Two full practical examples are provided at the end. Notebook ( colab ) Pre-class refresher activities and solution Summary card Lecture notes","title":"Support Vector Machines, the bias-variance tradeoff and a bit of kernel theory"},{"location":"svm.html#references","text":"On the general theory of SVMs for classification: A tutorial on Support Vector Machines for Pattern Recognition. C. J. C. Burges, Data Mining and Knowledge Discovery , 2 , 131-167, (1998). On support vector regression (and its extension to $\\nu$-SVR): A tutorial on Support Vector Regression. A. J. Smola and B. Sch\u00f6lkopf, Journal of Statistics and Computing , 14 (3), 199-222, (2004). New support vector algorithms. B. Sch\u00f6lkopf, A. J. Smola, R. C. Williamson, and P. L. Bartlett. Neural computation , 12 (5), 1207-1245, (2000). On One-Class SVMs: Support vector method for novelty detection. B. Sch\u00f6lkopf, R. C. Williamson, A. J. Smola, J. Shawe-Taylor, and John C. Platt. Neural Information Processing Systems , 12 , 582-588, (1999). On multi-class SVMs: On the algorithmic implementation of multiclass kernel-based vector machines. K. Crammer and Y. Singer. Journal of machine learning research , 2 , 265-292, (2001).","title":"References"},{"location":"trees.html","text":"Decision trees \ud83d\udd17","title":"Decision trees"},{"location":"trees.html#decision-trees","text":"","title":"Decision trees"},{"location":"unsup.html","text":"A quick tour of Unsupervised Learning \ud83d\udd17 Home Github repository This class links some important concepts seen in the previous classes of Statistics and the Introduction to Statistical Learning, with the concept of Unsupervised Learning and its specificities. Notebook ( colab )","title":"A quick tour of Unsupervised Learning"},{"location":"unsup.html#a-quick-tour-of-unsupervised-learning","text":"Home Github repository This class links some important concepts seen in the previous classes of Statistics and the Introduction to Statistical Learning, with the concept of Unsupervised Learning and its specificities. Notebook ( colab )","title":"A quick tour of Unsupervised Learning"},{"location":"xai.html","text":"Explainability in ML \ud83d\udd17","title":"Explainability in ML"},{"location":"xai.html#explainability-in-ml","text":"","title":"Explainability in ML"},{"location":"xgboost.html","text":"Gradient Boosting and XGBoost \ud83d\udd17","title":"Gradient Boosting and XGBoost"},{"location":"xgboost.html#gradient-boosting-and-xgboost","text":"","title":"Gradient Boosting and XGBoost"}]}